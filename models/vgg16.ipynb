{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './../datasets/dataset1'  # Update this to your dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "num_classes = 5\n",
    "validation_split = 0.2  # 20% of the data will be used for validation\n",
    "test_split = 0.1\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=validation_split + test_split\n",
    ")\n",
    "\n",
    "# Generators for training and validation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=validation_split / (validation_split + test_split)\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Data preparation for testing\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=test_split / (validation_split + test_split)\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the ResNet50 model, excluding the top layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    return tf.py_function(\n",
    "        lambda y_true, y_pred: balanced_accuracy_score(\n",
    "            y_true.numpy(), y_pred.numpy()),\n",
    "        (y_true, y_pred),\n",
    "        tf.float64)\n",
    "\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.argmax(y_true, axis=1), tf.int32)\n",
    "    y_pred = tf.cast(tf.argmax(y_pred, axis=1), tf.int32)\n",
    "\n",
    "    def compute_fscore(y_true, y_pred):\n",
    "        _, _, fscore, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='macro', zero_division=0)\n",
    "        return fscore\n",
    "\n",
    "    fscore = tf.py_function(\n",
    "        compute_fscore, (y_true, y_pred), tf.float64)\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, roc_auc_score, log_loss\n",
    "\n",
    "# Define custom F1 score metrics\n",
    "def f1_macro(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    def compute_f1(y_true, y_pred):\n",
    "        y_true = y_true.numpy()\n",
    "        y_pred = y_pred.numpy()\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "        return f1\n",
    "\n",
    "    f1 = tf.py_function(compute_f1, (y_true, y_pred), tf.float64)\n",
    "    return f1\n",
    "\n",
    "def f1_weighted(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    def compute_f1(y_true, y_pred):\n",
    "        y_true = y_true.numpy()\n",
    "        y_pred = y_pred.numpy()\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        return f1\n",
    "\n",
    "    f1 = tf.py_function(compute_f1, (y_true, y_pred), tf.float64)\n",
    "    return f1\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision, Recall, F1 Score\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # ROC-AUC (One-vs-Rest)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')\n",
    "    \n",
    "    # Log Loss\n",
    "    logloss = log_loss(y_true, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'roc_auc': roc_auc,\n",
    "        'log_loss': logloss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "learning_rate = 3.9e-5\n",
    "metrics = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.CategoricalAccuracy(),\n",
    "    tf.keras.metrics.AUC(),\n",
    "    balanced_accuracy,\n",
    "    fscore,\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall()\n",
    "]\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=metrics)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = history.history['val_accuracy']\n",
    "train_loss = history.history['accuracy']\n",
    "print(epochs, len(val_loss))\n",
    "\n",
    "num_epochs = range(0, epochs)\n",
    "plt.plot(num_epochs, val_loss, \"b\",\n",
    "         label=\"Validation loss\")\n",
    "plt.plot(num_epochs, train_loss, \"b+\",\n",
    "         label=\"Train loss\")\n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Make predictions on the test set\n",
    "y_true = test_generator.classes\n",
    "y_pred = model.predict(test_generator, steps=test_generator.samples // batch_size)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate additional metrics\n",
    "metrics = evaluate_model(y_true, y_pred_classes, y_pred)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Unfreeze some layers of the base model for fine-tuning\n",
    "# for layer in base_model.layers[:143]:\n",
    "#     layer.trainable = False\n",
    "# for layer in base_model.layers[143:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# # Recompile the model\n",
    "# model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Continue training\n",
    "# model.fit(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=train_generator.samples // batch_size,\n",
    "#     validation_steps=validation_generator.samples // batch_size,\n",
    "#     validation_data=validation_generator,\n",
    "#     epochs=epochs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test accuracy: {test_accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
