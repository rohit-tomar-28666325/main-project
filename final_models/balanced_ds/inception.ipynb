{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, cohen_kappa_score\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "REDUCED_SAMPLES_PER_CLASS = 2000\n",
    "epochs = 20\n",
    "learning_rate = 3.9e-5\n",
    "num_classes = 5\n",
    "folder_path = './../../datasets/dataset1'\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def preprocess_image(img_path, img_height, img_width):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# Function to create TensorFlow dataset\n",
    "def create_dataset(image_paths, labels, img_height, img_width, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: (preprocess_image(x, img_height, img_width), y))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # Improve performance\n",
    "    return dataset\n",
    "\n",
    "# Function to get image paths and labels with specific sampling for class 0 and 2\n",
    "def get_image_paths_and_labels(directory, reduced_samples_per_class):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(directory))\n",
    "    class_to_index = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_folder = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_folder):\n",
    "            class_images = [os.path.join(class_folder, filename) for filename in os.listdir(class_folder)]\n",
    "            if class_name in ['0', '2']:  # Reduce images to 3000 for class 0 and 2\n",
    "                class_images = np.random.choice(class_images, reduced_samples_per_class, replace=False)\n",
    "            image_paths.extend(class_images)\n",
    "            labels.extend([class_to_index[class_name]] * len(class_images))\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# Load image paths and labels\n",
    "image_paths, labels = get_image_paths_and_labels(folder_path, REDUCED_SAMPLES_PER_CLASS)\n",
    "\n",
    "# Convert labels to NumPy array\n",
    "labels = np.array(labels).reshape(-1, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "labels_onehot = encoder.fit_transform(labels)\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "dataset = create_dataset(image_paths, labels_onehot, IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets using scikit-learn\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(image_paths, labels_onehot, test_size=0.4, random_state=42, stratify=labels)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Create TensorFlow datasets for training, validation, and testing\n",
    "train_dataset = create_dataset(X_train, y_train, IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE)\n",
    "val_dataset = create_dataset(X_val, y_val, IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE)\n",
    "test_dataset = create_dataset(X_test, y_test, IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    return tf.py_function(\n",
    "        func=lambda y_true, y_pred: balanced_accuracy_score(\n",
    "            y_true.numpy(), y_pred.numpy()),\n",
    "        inp=[y_true, y_pred],\n",
    "        Tout=tf.float64)\n",
    "\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.argmax(y_true, axis=1), tf.int32)\n",
    "    y_pred = tf.cast(tf.argmax(y_pred, axis=1), tf.int32)\n",
    "\n",
    "    def compute_fscore(y_true, y_pred):\n",
    "        _, _, fscore, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='macro', zero_division=0)\n",
    "        return fscore\n",
    "\n",
    "    return tf.py_function(func=compute_fscore, inp=[y_true, y_pred], Tout=tf.float64)\n",
    "\n",
    "# Custom Kappa Metric\n",
    "def kappa_score(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    kappa_score = tf.py_function(func=cohen_kappa_score, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "    return kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.AUC(),\n",
    "    balanced_accuracy,\n",
    "    fscore,\n",
    "    kappa_score,\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message=\"A single label was found in y_true and y_pred.\")\n",
    "warnings.filterwarnings('ignore', message=\"y_pred contains classes not in y_true\") \n",
    "warnings.filterwarnings('ignore', message=\"A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\") \n",
    "\n",
    "\n",
    "def accuracyGraph(history):\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    print ('train_accuracy-> ', train_accuracy)\n",
    "    print ('val_accuracy-> ', val_accuracy)\n",
    "    print ('train_loss-> ', train_loss)\n",
    "    print ('val_loss-> ', val_loss)\n",
    "\n",
    "    epochs_no = range(len(train_accuracy) + 1)\n",
    "\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs_no, [0] + train_accuracy, 'b', label='Train Accuracy')\n",
    "    plt.plot(epochs_no, [0] + val_accuracy, 'r', label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs_no, [0] + train_loss, 'b', label='Train Loss')\n",
    "    plt.plot(epochs_no, [0] + val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs_no, [0] + val_accuracy, 'b')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs_no, [0] + val_loss, 'r')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def show_confusion_matrix(model):\n",
    "    y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "    if y_true.ndim > 1 and y_true.shape[1] > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    print(classification_report(y_true, y_pred_classes, target_names=[str(i) for i in range(num_classes)], zero_division=0))\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    def plot_confusion_matrix(cm, class_names):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    plot_confusion_matrix(conf_matrix, [str(i) for i in range(num_classes)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"categorical_crossentropy\", metrics=metrics)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyGraph(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_metrices = model.evaluate(test_dataset)\n",
    "print(\"Test Metrices\", _metrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Fine-tuning: Unfreeze some layers of the base model\n",
    "for layer in base_model.layers[:249]:   \n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss=\"categorical_crossentropy\", metrics=metrics)\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyGraph(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_metrices = model.evaluate(test_dataset)\n",
    "print(\"Test Metrices\", _metrices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
