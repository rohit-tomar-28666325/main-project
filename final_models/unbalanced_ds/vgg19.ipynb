{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, cohen_kappa_score\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "learning_rate = 3.9e-5\n",
    "# Parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "folder_path = './../../datasets/dataset1'\n",
    "num_classes = 5\n",
    "validation_split = 0.2\n",
    "test_split = 0.2  # Proportion of the data to use for testing\n",
    "\n",
    "# Load image file paths and their corresponding labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "class_names = sorted(os.listdir(folder_path))\n",
    "class_indices = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(folder_path, class_name)\n",
    "    for file_name in os.listdir(class_path):\n",
    "        file_path = os.path.join(class_path, file_name)\n",
    "        image_paths.append(file_path)\n",
    "        labels.append(class_indices[class_name])\n",
    "\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=test_split, stratify=labels, random_state=42)\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(train_paths, train_labels, test_size=validation_split, stratify=train_labels, random_state=42)\n",
    "\n",
    "# Function to load and preprocess the images\n",
    "def preprocess_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    image = image / 255.0  # Normalize to [0,1]\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    return image, label\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_dataset = train_dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_paths)).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "val_dataset = val_dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "test_dataset = test_dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    return tf.py_function(\n",
    "        func=lambda y_true, y_pred: balanced_accuracy_score(\n",
    "            y_true.numpy(), y_pred.numpy()),\n",
    "        inp=[y_true, y_pred],\n",
    "        Tout=tf.float64)\n",
    "\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.argmax(y_true, axis=1), tf.int32)\n",
    "    y_pred = tf.cast(tf.argmax(y_pred, axis=1), tf.int32)\n",
    "\n",
    "    def compute_fscore(y_true, y_pred):\n",
    "        _, _, fscore, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='macro', zero_division=0)\n",
    "        return fscore\n",
    "\n",
    "    return tf.py_function(func=compute_fscore, inp=[y_true, y_pred], Tout=tf.float64)\n",
    "\n",
    "# Custom Kappa Metric\n",
    "def kappa_score(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    kappa_score = tf.py_function(func=cohen_kappa_score, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "    return kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.AUC(),\n",
    "    balanced_accuracy,\n",
    "    fscore,\n",
    "    kappa_score,\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message=\"A single label was found in y_true and y_pred.\")\n",
    "warnings.filterwarnings('ignore', message=\"y_pred contains classes not in y_true\") \n",
    "warnings.filterwarnings('ignore', message=\"A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\") \n",
    "\n",
    "\n",
    "def accuracyGraph(history):\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    print ('train_accuracy-> ', train_accuracy)\n",
    "    print ('val_accuracy-> ', val_accuracy)\n",
    "    print ('train_loss-> ', train_loss)\n",
    "    print ('val_loss-> ', val_loss)\n",
    "\n",
    "    epochs_no = range(len(train_accuracy) + 1)\n",
    "\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs_no, [0] + train_accuracy, 'b', label='Train Accuracy')\n",
    "    plt.plot(epochs_no, [0] + val_accuracy, 'r', label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs_no, [0] + train_loss, 'b', label='Train Loss')\n",
    "    plt.plot(epochs_no, [0] + val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs_no, [0] + val_accuracy, 'b')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs_no, [0] + val_loss, 'r')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def show_confusion_matrix(model):\n",
    "    y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "    if y_true.ndim > 1 and y_true.shape[1] > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    print(classification_report(y_true, y_pred_classes, target_names=[str(i) for i in range(num_classes)], zero_division=0))\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    def plot_confusion_matrix(cm, class_names):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    plot_confusion_matrix(conf_matrix, [str(i) for i in range(num_classes)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "# Add custom top layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(5, activation='softmax')(x)  # 5 classes for diabetic retinopathy\n",
    "model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"categorical_crossentropy\", metrics=metrics)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyGraph(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_metrices = model.evaluate(test_dataset)\n",
    "# _metrices = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
    "\n",
    "print(\"Test Metrices\", _metrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Fine-tuning: Unfreeze some layers of the base model\n",
    "for layer in base_model.layers[:15]:   \n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss=\"categorical_crossentropy\", metrics=metrics)\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyGraph(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_metrices = model.evaluate(test_dataset)\n",
    "print(\"Test Metrices\", _metrices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
